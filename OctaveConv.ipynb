{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octave Convolution\n",
    "My try with pyTorch with a case study of Octave Convolution from https://arxiv.org/pdf/1904.05049.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cat  frog   cat plane\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/home/ubuntu/research/data', train=True,\n",
    "                            download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=512,shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/home/ubuntu/research/data', train=False,\n",
    "                            download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,batch_size=1024,shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5,padding=2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5,padding=2)\n",
    "        self.fc1 = nn.Linear(16*8*8,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16*8*8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a class for our Octave convolution. We shall create it so that it can easily replace vanilla convolution. alpha is the portion of the channels that are low frequency.\n",
    "\n",
    "For all convolution layers (except first and last) alpha_in and alpha_out are 0.5\n",
    "For first convolution layer: alpha_in = 0 and alpha_out = 0.5\n",
    "For last convolution layer: alpha_in = 0.5 and alpha_out = 0.0\n",
    "We want to pack the output (hf and lf components) such that they hf and lf all of the same size. during forward() we shall unpack them to their respective sizes. This is so that we can use OctConv as is with other pyTorch modules like Relu, pool etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctConv(nn.Module): \n",
    "    def __init__(self, ch_in, ch_out, kernel_size, stride=1, alphas=[0.5,0.5], padding=0): \n",
    "        super(OctConv, self).__init__()\n",
    "\n",
    "        # get layer parameters \n",
    "        self.alpha_in, self.alpha_out = alphas\n",
    "        assert 0 <= self.alpha_in <= 1 and 0 <= self.alpha_in <= 1, \"Alphas must be in interval [0, 1]\"\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = (kernel_size - stride ) // 2 ## padding\n",
    "        \n",
    "        # Calculate the exact number of high/low frequency channels \n",
    "        self.ch_in_lf = int(self.alpha_in*ch_in)\n",
    "        self.ch_in_hf = ch_in - self.ch_in_lf\n",
    "        self.ch_out_lf = int(self.alpha_out*ch_out) \n",
    "        self.ch_out_hf = ch_out - self.ch_out_lf\n",
    "\n",
    "        # Create convolutional and other modules necessary\n",
    "        self.hasLtoL = self.hasLtoH = self.hasHtoL = self.hasHtoH = False\n",
    "        if (self.ch_in_lf and self.ch_out_lf):    \n",
    "            self.hasLtoL = True\n",
    "            self.conv_LtoL = nn.Conv2d(self.ch_in_lf, self.ch_out_lf, self.kernel_size, padding=self.padding)\n",
    "        if (self.ch_in_lf and self.ch_out_hf): \n",
    "            self.hasLtoH = True\n",
    "            self.conv_LtoH = nn.Conv2d(self.ch_in_lf, self.ch_out_hf, self.kernel_size, padding=self.padding)\n",
    "        if (self.ch_in_hf and self.ch_out_lf):\n",
    "            self.hasHtoL = True\n",
    "            self.conv_HtoL = nn.Conv2d(self.ch_in_hf, self.ch_out_lf, self.kernel_size, padding=self.padding)\n",
    "        if (self.ch_in_hf and self.ch_out_hf):\n",
    "            self.hasHtoH = True\n",
    "            self.conv_HtoH = nn.Conv2d(self.ch_in_hf, self.ch_out_hf, self.kernel_size, padding=self.padding)\n",
    "        self.avg_pool  = nn.AvgPool2d(2,2)\n",
    "        \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # Split input into high frequency and low frequency components\n",
    "        fmap_w = input.shape[-1]\n",
    "        fmap_h = input.shape[-2]\n",
    "        # We resize the high freqency components to the same size as the low frequency component when \n",
    "        # sending out as output. So when bringing in as input, we want to reshape it to have the original  \n",
    "        # size as the intended high frequnecy channel (if any high frequency component is available). \n",
    "        input_hf = input\n",
    "        if (self.ch_in_lf):\n",
    "            input_hf = input[:,:self.ch_in_hf*4,:,:].reshape(-1,self.ch_in_hf,fmap_h*2,fmap_w*2)\n",
    "            input_lf = input[:,self.ch_in_hf*4:,:,:]    \n",
    "        \n",
    "        # Create all conditional branches \n",
    "        LtoH = HtoH = LtoL = HtoL = 0.\n",
    "        if (self.hasLtoL):\n",
    "            LtoL = self.conv_LtoL(input_lf)\n",
    "        if (self.hasHtoH):\n",
    "            HtoH = self.conv_HtoH(input_hf)\n",
    "            op_h, op_w = HtoH.shape[-2]//2, HtoH.shape[-1]//2\n",
    "            HtoH = HtoH.reshape(-1, self.ch_out_hf*4, op_h, op_w)\n",
    "        if (self.hasLtoH):\n",
    "            LtoH = F.interpolate(self.conv_LtoH(input_lf), scale_factor=2, mode='bilinear')\n",
    "            op_h, op_w = LtoH.shape[-2]//2, LtoH.shape[-1]//2\n",
    "            LtoH = LtoH.reshape(-1, self.ch_out_hf*4, op_h, op_w)\n",
    "        if (self.hasHtoL):\n",
    "            HtoL = self.avg_pool(self.conv_HtoL(input_hf))\n",
    "        \n",
    "        # Elementwise addition of high and low freq branches to get the output\n",
    "        out_hf = LtoH + HtoH\n",
    "        out_lf = LtoL + HtoL\n",
    "        \n",
    "        if (self.ch_out_lf == 0):\n",
    "            return out_hf\n",
    "        if (self.ch_out_hf == 0):\n",
    "            return out_lf\n",
    "        op = torch.cat([out_hf,out_lf],dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create our network using our new convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_OctConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_OctConv, self).__init__()\n",
    "        self.conv1 = OctConv(3,6,5,alphas=[0.,0.5])\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = OctConv(6,16,5,alphas=[0.5,0.])\n",
    "        self.fc1 = nn.Linear(16*8*8,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.pool(F.relu(self.conv1(input)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16*8*8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #print(\"\\tIn Model: input size\", input.size(),\n",
    "        #      \"output size\", x.size())\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Net_OctConv(\n",
       "    (conv1): OctConv(\n",
       "      (conv_HtoL): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv_HtoH): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): OctConv(\n",
       "      (conv_LtoH): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv_HtoH): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (fc1): Linear(in_features=1024, out_features=120, bias=True)\n",
       "    (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net_OctConv()\n",
    "#net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 3, 32, 32]             228\n",
      "            Conv2d-2            [-1, 3, 32, 32]             228\n",
      "            Conv2d-3            [-1, 3, 32, 32]             228\n",
      "            Conv2d-4            [-1, 3, 32, 32]             228\n",
      "         AvgPool2d-5            [-1, 3, 16, 16]               0\n",
      "         AvgPool2d-6            [-1, 3, 16, 16]               0\n",
      "           OctConv-7           [-1, 15, 16, 16]               0\n",
      "           OctConv-8           [-1, 15, 16, 16]               0\n",
      "         MaxPool2d-9             [-1, 15, 8, 8]               0\n",
      "        MaxPool2d-10             [-1, 15, 8, 8]               0\n",
      "           Conv2d-11           [-1, 16, 16, 16]           1,216\n",
      "           Conv2d-12           [-1, 16, 16, 16]           1,216\n",
      "           Conv2d-13             [-1, 16, 8, 8]           1,216\n",
      "           Conv2d-14             [-1, 16, 8, 8]           1,216\n",
      "          OctConv-15             [-1, 64, 8, 8]               0\n",
      "          OctConv-16             [-1, 64, 8, 8]               0\n",
      "        MaxPool2d-17             [-1, 64, 4, 4]               0\n",
      "        MaxPool2d-18             [-1, 64, 4, 4]               0\n",
      "           Linear-19                  [-1, 120]         123,000\n",
      "           Linear-20                  [-1, 120]         123,000\n",
      "           Linear-21                   [-1, 84]          10,164\n",
      "           Linear-22                   [-1, 84]          10,164\n",
      "           Linear-23                   [-1, 10]             850\n",
      "           Linear-24                   [-1, 10]             850\n",
      "      Net_OctConv-25                   [-1, 10]               0\n",
      "      Net_OctConv-26                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 273,804\n",
      "Trainable params: 273,804\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.34\n",
      "Params size (MB): 1.04\n",
      "Estimated Total Size (MB): 1.39\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      " 55%|█████▌    | 11/20 [01:16<00:58,  6.53s/it]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "for epoch in tqdm(range(20)):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        inputs_cpu, labels_cpu = data\n",
    "        inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        #print(\"Outside: input size\", inputs.size(),\n",
    "        #  \"output_size\", outputs.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch +1, i+1, running_loss/2000))\n",
    "            running_loss = 0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs_cpu, labels_cpu = data\n",
    "        inputs, labels = inputs_cpu.to(device), labels_cpu.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print ('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
